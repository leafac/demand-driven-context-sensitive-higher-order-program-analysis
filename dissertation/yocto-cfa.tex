% !TEX program = lualatex
% !TEX options = -shell-escape -synctex=1 -interaction=nonstopmode -file-line-error "%DOC%"
% !BIB program = bibtex

\documentclass[12pt, oneside]{book}

\usepackage[a-1b]{pdfx}
\hypersetup{hidelinks, bookmarksnumbered}
\usepackage{tocbibind}

\usepackage[top = 1in, right = 1in, bottom = 1in, left = 1.5in]{geometry}
\usepackage[doublespacing]{setspace}

\pagestyle{plain}

\usepackage{fontspec, unicode-math}
\setmainfont{XCharter}
\setmonofont{FiraMono}[Scale = 0.9]
\setmathfont{TeX Gyre Pagella Math}

\usepackage{minted}
\usemintedstyle{vs}
\setminted{fontsize = \footnotesize, baselinestretch = 1}
\setmintedinline{fontsize = \normalsize}

\begin{document}

\frontmatter

\begin{center}
\begin{singlespace}
\vspace*{0.5in}

\textbf{\uppercase{Yocto-CFA}}

\vspace*{1in}

by

Leandro Facchinetti

\vspace*{1.5in}

A dissertation submitted to Johns Hopkins University\\in conformity with the requirements for the degree of Doctor of Philosophy

\vspace*{0.5in}

Baltimore, Maryland

August 2020
\end{singlespace}
\end{center}

\thispagestyle{empty}
\clearpage

\chapter{Abstract}

% TODO

\paragraph{Primary Reader and Advisor:}

Dr.~Scott Fraser Smith.

\paragraph{Readers:}

Dr.~Zachary Eli Palmer and Dr.~Matthew Daniel Green.

\chapter{Acknowledgements}

% TODO

\tableofcontents
\listoftables
\listoffigures

\mainmatter

% TODO: Introduction

\chapter{Developing an Analyzer}

% TODO: An overview of the rest of the section

\section{The Analyzed Language: Yocto-JavaScript}

\paragraph{The Importance of Choosing the Analyzed Language.}

Our first decision when developing an analyzer is which language it should analyze. This decision is important because the analyzed language may affect the analyzer’s precision and running time. Consider, for example, an analysis technique called \(k\)-CFA~\cite{k-cfa}: an analyzer using \(k\)-CFA on a language with higher-order functions may be slower than an analyzer using \(k\)-CFA on a language with objects (the algorithmic complexity of the former is exponential, of the latter, polynomial)~\cite{m-cfa}.

Our goal is to develop an analyzer for a language with higher-order functions, so an ideal analyzed …

Ideally, the analyzed language would be a real-world language, for example, JavaScript or Java. Unfortunately, these languages have too many features, and an analyzer attempting to support of all of them would end up being too complex.

Our options of analyzed language are:

\begin{enumerate}
\item \textbf{An artificial little language of our own design.} This is the approach that most papers take. Its benefit is that the analyzed language can be tailored for the kinds of discussions the authors want to have in the paper, but this approach has many drawbacks: the language is unfamiliar to the readers; it becomes impossible to reuse existing infrastructure, for example, parsers, pretty-printers, and so forth; and the language may not reflect the features that programmers actually use.

On top of that, perhaps the biggest issue with this approach has to do with \emph{evaluating} the analyzer. Often the only code in an artificial little language is written by the authors of the analyzer themselves, and this code tends to be only small programs that do not exercise all the complex behavior an analyzer may exhibit. One common solution to this problem is to develop a compiler from an existing real-world language to the analyzed language, but this comes with its own problems: it is more engineering effort; the compiler may introduce artifacts that affect the analyzer’s performance; and it becomes more difficult to make sense of the analyzer’s outputs in terms of the original real-world language.

Another issue with this approach is that it requires effort to be put into language design, but this is not a significant disadvantage in relation to the other approaches below, because they require comparable effort to be put into choosing a real-world language and the appropriate subset of features to analyze.

\item \textbf{A real-world language.} This approach would be ideal if it were practical, but real-world languages have too many features and the resulting analyzer would be too complex. Some analyzers work around this by simply ignoring language features that are considered too obscure and that would complicate the analysis too much. The most notable example of problematic feature is arbitrary code execution, for example, the \mintinline{js}{eval()} function in JavaScript that runs arbitrary code represented as a string~\cite{eval}. The problem with features such as \mintinline{js}{eval()} is that the analyzer would not have access to the code that it was supposed to analyze, so to remain sound (see §~\ref{TODO: Section in the Introduction about soundness and _soundiness_}) it would have to be conservative and produce results that would end up being too imprecise to be useful. An example of an analyzer that takes this approach is JSAI~\cite{jsai}, an analyzer for JavaScript that does not support \mintinline{js}{eval()}.

But even if we leave aside problematic features such as \mintinline{js}{eval()}, real-world languages are still very complex. Some tools cope with this complexity by compiling programs in the real-world language into an artificial little language of their own design. As discussed above, this strategy has a few issues, but it is helpful nonetheless because the accidental complexity can be resolved by the compiler, leaving the analyzer to handle only the complexity that is essential to the analyzed language. Deciding whether some feature is accidental or essential complexity is a fine art that requires significant effort. One example of a tool that takes this approach is \(\lambda_{\mathit{JS}}\)~\cite{lambda-js}.

\item \textbf{A subset of the features from a real-world language.}
\end{enumerate}

We choose option 3, which means we have to select a real-world language and then the subset of features from this language that we want to support in our analyzer.

\paragraph{A Real-World Language to Analyze: JavaScript.}

From all the real-world languages we could choose, JavaScript is the best option. First, because it fulfills the prerequisite: it includes higher-order functions. Second, because it is the most used programming language in the world~\cite{stack-overflow-developer-survey, jet-brains-developer-survey}

\appendix

% TODO

\backmatter

\bibliographystyle{plain}
\bibliography{\jobname}

\chapter{Biographical Statement}

% TODO

\end{document}
