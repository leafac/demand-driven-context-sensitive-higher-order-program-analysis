% !TEX program = lualatex
% !TEX options = -shell-escape -synctex=1 -interaction=nonstopmode -file-line-error "%DOC%"
% !BIB program = bibtex

\documentclass[12pt, oneside]{book}

\usepackage[a-1b]{pdfx}
\hypersetup{hidelinks, bookmarksnumbered}
\usepackage{tocbibind}

\usepackage[top = 1in, right = 1in, bottom = 1in, left = 1.5in]{geometry}
\usepackage[doublespacing]{setspace}

\pagestyle{plain}

\usepackage{fontspec, unicode-math}
\setmainfont{XCharter}
\setmonofont{FiraMono}[Scale = 0.9]
\setmathfont{TeX Gyre Pagella Math}

\usepackage{minted}
\usemintedstyle{vs}
\setminted{fontsize = \footnotesize, baselinestretch = 1}
\setmintedinline{fontsize = \normalsize}

\begin{document}

\frontmatter

\begin{center}
\begin{singlespace}
\vspace*{0.5in}

\textbf{\uppercase{Yocto-CFA}}

\vspace*{1in}

by

Leandro Facchinetti

\vspace*{1.5in}

A dissertation submitted to Johns Hopkins University\\in conformity with the requirements for the degree of Doctor of Philosophy

\vspace*{0.5in}

Baltimore, Maryland

August 2020
\end{singlespace}
\end{center}

\thispagestyle{empty}
\clearpage

\chapter{Abstract}

% TODO

\paragraph{Primary Reader and Advisor:}

Dr.~Scott Fraser Smith.

\paragraph{Readers:}

Dr.~Zachary Eli Palmer and Dr.~Matthew Daniel Green.

\chapter{Acknowledgements}

% TODO

\tableofcontents
\listoftables
\listoffigures

\mainmatter

% TODO: Introduction

\chapter{Developing an Analyzer}

% TODO: An overview of the rest of the section

\section{The Analyzed Language: Yocto-JavaScript}

\paragraph{The Importance of Choosing the Analyzed Language.}

Our first decision when developing an analyzer is the language that it analyzes. This decision is important because the analyzed language determines how difficult it is to develop the analyzer—more features in the analyzed language require a more elaborate analyzer. Also, the analyzed language may affect the analyzer’s precision and running time; for example, consider an analysis technique called \(k\)-CFA~\cite{k-cfa}: an analyzer using \(k\)-CFA on a language with higher-order functions may be slower than another analyzer using \(k\)-CFA on a language with objects (the algorithmic complexity of the former is exponential; the latter, polynomial)~\cite{m-cfa}.

Our options of analyzed language are:

\begin{enumerate}
\item \textbf{An artificial little language of our own design.} This is the approach that most papers take. Its benefit is that the analyzed language can be tailored for the kinds of discussions the authors want to have in the paper, but this approach has many drawbacks: the language is unfamiliar to the readers; it becomes impossible to reuse existing infrastructure, for example, parsers, pretty-printers, and so forth; and the language may not reflect the features that programmers actually use.

On top of that, perhaps the biggest issue with this approach has to do with \emph{evaluating} the analyzer. Often the only code in an artificial little language is written by the authors of the analyzer themselves, and this code tends to be only small programs that do not exercise all the complex behavior an analyzer may exhibit. One common solution to this problem is to develop a compiler from an existing real-world language to the analyzed language, but this comes with its own problems: it is more engineering effort; the compiler may introduce artifacts that affect the analyzer’s performance; and it becomes more difficult to make sense of the analyzer’s outputs in terms of the original real-world language.

Another issue with this approach is that it requires effort to be put into language design, but this is not a significant disadvantage in relation to the other approaches below, because they require comparable effort to be put into choosing a real-world language and the appropriate subset of features to analyze.

\item \textbf{A real-world language.} This approach would be the most realistic and useful if it were practical, but real-world languages have too many features which would make the resulting analyzer too complex. Some analyzers work around this by simply ignoring language features that are considered obscure and would complicate the analysis. The most notable example of such a feature is running arbitrary code at runtime, for example, the \mintinline{js}{eval()} function in JavaScript that runs an arbitrary string of code, and the Reflection API in Java. The problem with features like these is that the analyzer may not even have access to the code it is supposed to analyze—suppose, for example, that \mintinline{js}{eval()} is called with a string that comes from user input. In that case, the best the analyzer could do is expect the worse, for example, saying that the result of a call to \mintinline{js}{eval()} could be anything at all, but then the analysis outputs would be too imprecise to be useful. An example of a tool that attempts to analyze a real-world language is JSAI~\cite{jsai}, an analysis for JavaScript, and it does not support \mintinline{js}{eval()}.

Even when problematic features are left aside, real-world languages are still often too complex. One solution some tools find to manage this complexity is to compile the real-world language into a simpler language that is actually analyzed. This comes with all the technical difficulties mentioned above when discussing a compiler from a real-world language into an artificial little language of our design: it is often a big engineering effort; the analysis results have to be translated back into terms of the real-world language in which the original program was written for the programmers to make sense of them; and so forth. Still, this approach is often feasible because it breaks apart the complexity into different components: the compiler, the analyzer, the component that translates results back, and so forth. How much complexity goes into each component is a design choice in a space that is often difficult to navigate. An example of analysis that does this is \(\lambda_{\mathit{JS}}\), an analysis for JavaScript~\cite{lambda-js}.

\item \textbf{A subset of the features from a real-world language.}
\end{enumerate}

We choose option 3, which means we have to select a real-world language and then the subset of features from this language that we want to support in our analyzer.

\paragraph{A Real-World Language to Analyze: JavaScript.}

From all the real-world languages we could choose, JavaScript is the best option. First, because it fulfills the prerequisite: it includes higher-order functions. Second, because it is the most used programming language in the world~\cite{stack-overflow-developer-survey, jet-brains-developer-survey}

\appendix

% TODO

\backmatter

\bibliographystyle{plain}
\bibliography{\jobname}

\chapter{Biographical Statement}

% TODO

\end{document}
